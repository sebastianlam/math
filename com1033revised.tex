\documentclass{article}
\author{Jim S. Lam}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usetikzlibrary{automata, positioning, arrows}

\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\begin{document}
\title{COM1033 FOUNDATIONS OF COMPUTING II}
\maketitle
\tableofcontents
\pagebreak

\section{Vectors}
\subsection{Vector Definition}

Let $n \in \mathbb{N}$ and $n > 0$.

The set of all vectors is the Cartesian product of $\mathbb{R}$ by $n$ times, which is a set of ordered $n$-tuples of real numbers.

\begin{align*}
    \text{For instance, a vector in } \mathbb{R}^3 \text{ is defined as:} \\
    \mathbb{R}^3 = \{ (x, y, z) \mid x, y, z \in \mathbb{R} \}
\end{align*}

\subsubsection*{Null Vector}

The null vector is a vector with all elements equal to 0.

\begin{align*}
    \vec{o} = \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}
\end{align*}

\subsection{Vector Operations}
\subsubsection{Addition}
\begin{align*}
    \begin{pmatrix} a \\ b \\ c \end{pmatrix} + \begin{pmatrix} d \\ e \\ f \end{pmatrix} = \begin{pmatrix} a + d \\ b + e \\ c + f \end{pmatrix}
\end{align*}
\subsubsection{Scalar Multiplication}
\begin{align*}
    \lambda
    \begin{pmatrix}
        a \\ b \\ c
    \end{pmatrix}
    =
    \begin{pmatrix}
        \lambda a \\ \lambda b \\ \lambda c
    \end{pmatrix}
\end{align*}
\subsubsection{Dot Product / Scalar Product}
\begin{align*}
    \vec{a} \cdot \vec{b} = a_1 b_1 + a_2 b_2 + ... + a_n b_n
\end{align*}

\subsubsection*{Examples}
\begin{enumerate}
    \item Sum the following vectors $\in \mathbb{R}^3$:
          \begin{align*}
              \vec{v_1}             & = \begin{pmatrix} 3 \\ 5 \\ -4 \end{pmatrix}, \vec{v_2} = \begin{pmatrix} 0 \\ 1 \\ 4 \end{pmatrix} \\
              \vec{v_1} + \vec{v_2} & = \begin{pmatrix} 3 \\ 6 \\ 0 \end{pmatrix}
          \end{align*}
    \item Calculate the product $\lambda \vec{v_1}$ with $\lambda = 2$:
          \begin{align*}
              \lambda \vec{v_1} & = 2 \begin{pmatrix} 3 \\ 5 \\ -4 \end{pmatrix} = \begin{pmatrix} 6 \\ 10 \\ -8 \end{pmatrix}
          \end{align*}
    \item Compute the dot product of $\vec{u}$ and $\vec{v}$:
          \begin{align*}
              \vec{u}               & = \begin{pmatrix} 3 \\ 5 \\ -4 \end{pmatrix}, \vec{v} = \begin{pmatrix} 2 \\ 2 \\ 4 \end{pmatrix} \\
              \vec{u} \cdot \vec{v} & = 3 \cdot 2 + 5 \cdot 2 + (-4) \cdot 4 = 6 + 10 - 16 = 0
          \end{align*}
\end{enumerate}

\section{Linear Dependence / Independence}
\subsection{Linear Combination}
Let $\lambda_1, \lambda_2, ..., \lambda_n$ be $n$ scalars, and $\vec{v_1}, \vec{v_2}, ..., \vec{v_n}$ be $n$ vectors.
\begin{align*}
    \vec{w} = \lambda_1 \vec{v_1} + \lambda_2 \vec{v_2} + ... + \lambda_n \vec{v_n}
\end{align*}
$\vec{w}$ is a linear combination of $\vec{v_1}, \vec{v_2}, ..., \vec{v_n}$ using the scalars $\lambda_1, \lambda_2, ..., \lambda_n$.

\subsection{Linear Dependence}

Let there be $n$ vectors of the same dimension.

If the null vector $\vec{o}$ can be expressed as a linear combination of the $n$ vectors as defined, using non-null scalars, then the $n$ vectors are linearly dependent.

In other words, the $n$ vectors are linearly dependent if:

\begin{align*}
    \vec{o} = \lambda_1 \vec{v_1} + \lambda_2 \vec{v_2} + ... + \lambda_n \vec{v_n}, \text{ with } \exists \lambda_i \neq 0, i = 1, 2, ..., n
\end{align*}

\subsubsection*{Examples}
\begin{enumerate}
    \item Let $\vec{v_1} = \begin{pmatrix} 1 \\ 2 \\ 1 \end{pmatrix}$, $\vec{v_2} = \begin{pmatrix} 0 \\ 2 \\ 2 \end{pmatrix}$, and $\vec{v_3} = \begin{pmatrix} 1 \\ 6 \\ 5 \end{pmatrix}$. Show that these vectors are linearly dependent by finding scalars $\lambda_1, \lambda_2, \lambda_3$ such that $\lambda_1 \vec{v_1} + \lambda_2 \vec{v_2} + \lambda_3 \vec{v_3} = \vec{0}$.

          \begin{align*}
              \text{When: } \lambda_1 = 1, \lambda_2 = 2, \lambda_3 = -1,                                                   \\
              \lambda_1 \vec{v_1} + \lambda_2 \vec{v_2} + \lambda_3 \vec{v_3} & = \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}
          \end{align*}
          Therefore, the vectors $\vec{v_1}$, $\vec{v_2}$, and $\vec{v_3}$ are linearly dependent.

    \item Let $v_1, v_2, ..., v_n$ be $n$ linearly independent vectors. Consider the set of scalars $\lambda_1, \lambda_2, ..., \lambda_n$ such that $\lambda_1 v_1 + \lambda_2 v_2 + ... + \lambda_n v_n = 0$. Find alternative sets of scalars that satisfy this equation.

          \begin{align*}
              \text{We can multiply all the scalars by a common scaling factor, let's say } \mu, \\
              \mu \lambda_1 v_1 + \mu \lambda_2 v_2 + ... + \mu \lambda_n v_n & = 0
          \end{align*}
          Therefore, $\mu \lambda_1, \mu \lambda_2, ..., \mu \lambda_n$ is an alternative set of scalars that satisfies the equation.
\end{enumerate}

\section{Matrices}

\subsection{Matrix Multiplication}

Let $A$ be a $m \times n$ matrix, and $B$ be a $n \times p$ matrix.

\begin{align*}
    \mathbf{A} \times \mathbf{B} = \mathbf{C} \\
    \begin{pmatrix}
        a_{1,1} & a_{1,2} & ... & a_{1,n} \\
        a_{2,1} & a_{2,2} & ... & a_{2,n} \\
        ...     & ...     & ... & ...     \\
        a_{m,1} & a_{m,2} & ... & a_{m,n} \\
    \end{pmatrix}
    \times
    \begin{pmatrix}
        b_{1,1} & b_{1,2} & ... & b_{1,p} \\
        b_{2,1} & b_{2,2} & ... & b_{2,p} \\
        ...     & ...     & ... & ...     \\
        b_{n,1} & b_{n,2} & ... & b_{n,p} \\
    \end{pmatrix}
    =
    \begin{pmatrix}
        c_{1,1} & c_{1,2} & ... & c_{1,p} \\
        c_{2,1} & c_{2,2} & ... & c_{2,p} \\
        ...     & ...     & ... & ...     \\
        c_{m,1} & c_{m,2} & ... & c_{m,p} \\
    \end{pmatrix}
\end{align*}

Where $c_{i,j} = \sum_{k=1}^n a_{i,k} b_{k,j}$.

\subsubsection*{Example}
\begin{align*}
    \begin{pmatrix}
        1 & 0 & 2 \\
        3 & 5 & 1 \\
        2 & 2 & 0 \\
    \end{pmatrix}
    \begin{pmatrix}
        1 & 0  & 3 \\
        1 & -1 & 0 \\
        4 & 2  & 1 \\
    \end{pmatrix}
     & =
    \begin{pmatrix}
        0 - 2 - 0 + 0 + 12 - 20     & -10                                         \\
        -1 -(0) -(0) + 0 + 6 -(-12) & 17                                          \\
        v_1 + 2v_2 - v_3            & \begin{pmatrix} -1 \\ 3 \\ -1 \end{pmatrix}
    \end{pmatrix}
\end{align*}

\subsection{Determinant of a Matrix}

The determinant of a $3 \times 3$ matrix is a scalar value that can be computed using the Laplace expansion along the first row:

\begin{align*}
    \det \begin{pmatrix}
             a_{11} & a_{12} & a_{13} \\
             a_{21} & a_{22} & a_{23} \\
             a_{31} & a_{32} & a_{33}
         \end{pmatrix}
     & = a_{11} \begin{vmatrix}
                    a_{22} & a_{23} \\
                    a_{32} & a_{33}
                \end{vmatrix}
    - a_{12} \begin{vmatrix}
                 a_{21} & a_{23} \\
                 a_{31} & a_{33}
             \end{vmatrix}
    + a_{13} \begin{vmatrix}
                 a_{21} & a_{22} \\
                 a_{31} & a_{32}
             \end{vmatrix}
\end{align*}

\subsubsection*{Example}
Consider the matrix:
\begin{align*}
    \mathbb{M} =
    \begin{pmatrix}
        1 & 0 & 2 \\
        3 & 5 & 1 \\
        2 & 2 & 0
    \end{pmatrix}
\end{align*}

\begin{align*}
    \det(\mathbb{M}) & = 1 \begin{vmatrix}
                               5 & 1 \\
                               2 & 0
                           \end{vmatrix}
    - 0 \begin{vmatrix}
            3 & 1 \\
            2 & 0
        \end{vmatrix}
    + 2 \begin{vmatrix}
            3 & 5 \\
            2 & 2
        \end{vmatrix}                                                                                                       \\
                     & = 1 \cdot (5 \cdot 0 - 1 \cdot 2) - 0 \cdot (3 \cdot 0 - 1 \cdot 2) + 2 \cdot (3 \cdot 2 - 5 \cdot 2) \\
                     & = 6 - 6 - 0 + 6 + 0 - 12                                                                              \\
                     & = -6
\end{align*}

Therefore, the determinant of $\mathbb{M}$ is $-6$.

\subsection{Properties of Determinants}
Some important properties of determinants include:

\begin{enumerate}
    \item If a matrix has a row (or column) of all zeros, then its determinant is 0.
    \item If two rows (or columns) of a matrix are identical, then its determinant is 0.
    \item If two rows (or columns) of a matrix are scalar multiples of each other, then its determinant is 0.
    \item Interchanging two rows (or columns) of a matrix changes the sign of its determinant.
    \item The determinant of a product of two matrices is equal to the product of their determinants: $\det(AB) = \det(A) \det(B)$.
\end{enumerate}

%
\section{Determinants and Odd-Even Permutations}

\subsection{Permutations}

A permutation of $n$ objects is a rearrangement of their order. For a matrix $A_{n\times n}$, a permutation of rows or columns corresponds to a rearrangement of the rows or columns.

\subsection{Odd and Even Permutations}

A permutation is called \textbf{even} if it can be obtained from the original order by an even number of transpositions (swaps of two elements). Otherwise, it is called \textbf{odd}.

\subsection{Effect on Determinant}

\begin{quotation}
\noindent\textbf{Theorem:} Applying an odd permutation to the rows or columns of a matrix $A$ changes the sign of $\det(A)$. Applying an even permutation leaves the sign of $\det(A)$ unchanged.
\end{quotation}

\subsection{Computing Determinants Using Permutations}

The determinant of an $n\times n$ matrix $A$ can be expressed as a sum over all permutations $\pi$ of the set $\{1, 2, \ldots, n\}$:

\begin{align*}
\det(A) = \sum_{\pi} \epsilon_{\pi} a_{1\pi(1)}a_{2\pi(2)}\cdots a_{n\pi(n)}
\end{align*}

where $\epsilon_{\pi}$ is $+1$ if $\pi$ is even, and $-1$ if $\pi$ is odd.

This formula follows directly from the Laplace expansion formula for determinants and the effect of permutations on the sign of the determinant.

\subsection{Row and Column Swaps}

Swapping any two rows or columns of a matrix corresponds to an odd permutation. Therefore, swapping two rows or columns changes the sign of the determinant.
% 

\subsubsection*{Examples}
\begin{enumerate}
    \item Determine if the following matrix is invertible by computing its determinant:
          \begin{align*}
              \begin{pmatrix}
                  1 & 1  & 0  & 1 \\
                  4 & 1  & -1 & 0 \\
                  2 & -1 & 1  & 2
              \end{pmatrix}
          \end{align*}

          We can perform row operations to transform the matrix into an upper triangular form:
          \begin{align*}
              \begin{pmatrix}
                  1 & 1  & 0  & 1 \\
                  4 & 1  & -1 & 0 \\
                  2 & -1 & 1  & 2
              \end{pmatrix}
              \xrightarrow{R_3 \leftarrow R_3 - 2R_1}
              \begin{pmatrix}
                  1 & 1  & 0  & 1 \\
                  4 & 1  & -1 & 0 \\
                  0 & -3 & 1  & 0
              \end{pmatrix}
              \xrightarrow{R_2 \leftarrow R_2 - 4R_1}
              \begin{pmatrix}
                  1 & 1  & 0  & 1  \\
                  0 & -3 & -1 & -4 \\
                  0 & -3 & 1  & 0
              \end{pmatrix}
          \end{align*}

          Now, the determinant is simply the product of the diagonal entries: $\det = 1 \cdot (-3) \cdot 1 = -3 \neq 0$. Therefore, the matrix is invertible.

    \item Compute the determinants of $AB$ and $BA$ for the matrices:
          \begin{align*}
              A = \begin{pmatrix}
                      1 & 0 & 2 \\
                      3 & 5 & 1 \\
                      2 & 2 & 0
                  \end{pmatrix}, \quad
              B = \begin{pmatrix}
                      1 & 0  & 3 \\
                      1 & -1 & 0 \\
                      4 & 2  & 1
                  \end{pmatrix}
          \end{align*}

          We have already computed $\det(A) = -6$ in a previous example. For $B$, we can use the Laplace expansion along the first row:

          \begin{align*}
              \det(B) & = 1 \begin{vmatrix}
                                -1 & 0 \\
                                2  & 1
                            \end{vmatrix}
              - 0 \begin{vmatrix}
                      1 & 0 \\
                      4 & 1
                  \end{vmatrix}
              + 3 \begin{vmatrix}
                      1 & -1 \\
                      4 & 2
                  \end{vmatrix}                                                                                                  \\
                      & = 1 \cdot (-1 \cdot 1 - 0 \cdot 2) - 0 \cdot (1 \cdot 1 - 4 \cdot 0) + 3 \cdot (1 \cdot 2 - 4 \cdot (-1)) \\
                      & = -1 + 0 + 3 \cdot 6                                                                                      \\
                      & = 17
          \end{align*}

          Therefore, $\det(B) = 17$. To compute $\det(AB)$ and $\det(BA)$, we can use the property $\det(AB) = \det(A) \det(B)$:

          \begin{align*}
              \det(AB) = \det(A) \det(B) = (-6) \cdot 17 = -102 \\
              \det(BA) = \det(B) \det(A) = 17 \cdot (-6) = -102
          \end{align*}

          Thus, $\det(AB) = \det(BA) = -102$.
\end{enumerate}

\section{Linear Equations}

\subsection{System of Linear Equations}

\begin{align*}
    \begin{cases}
        a_{1,1}x_1 + a_{1,2}x_2 + ... + a_{1,n}x_n = b_1 \\
        a_{2,1}x_1 + a_{2,2}x_2 + ... + a_{2,n}x_n = b_2 \\
        \vdots                                           \\
        a_{m,1}x_1 + a_{m,2}x_2 + ... + a_{m,n}x_n = b_m
    \end{cases}
\end{align*}

The above system of $m$ linear equations in $n$ variables can be expressed as a matrix equation:

\begin{align*}
    \begin{pmatrix}
        a_{1,1} & a_{1,2} & ...    & a_{1,n} \\
        a_{2,1} & a_{2,2} & ...    & a_{2,n} \\
        \vdots  & \vdots  & \ddots & \vdots  \\
        a_{m,1} & a_{m,2} & ...    & a_{m,n}
    \end{pmatrix}
    \begin{pmatrix}
        x_1    \\
        x_2    \\
        \vdots \\
        x_n
    \end{pmatrix}
    =
    \begin{pmatrix}
        b_1    \\
        b_2    \\
        \vdots \\
        b_m
    \end{pmatrix}
\end{align*}

\subsubsection{Cramer's Rule}

For a system of $n$ linear equations in $n$ variables, Cramer's rule provides a way to solve for the variables using determinants. Let $A$ be the coefficient matrix, $x$ be the vector of variables, and $b$ be the constant vector.

\begin{align*}
    Ax = b
\end{align*}

The solution for the $i$-th variable $x_i$ is given by:

\begin{align*}
    x_i = \frac{\det(A_i)}{\det(A)}
\end{align*}

Where $A_i$ is the matrix obtained by replacing the $i$-th column of $A$ with the constant vector $b$.

\subsubsection*{Example}
Consider the following system of linear equations:

\begin{align*}
    \begin{cases}
        3x - 2y + z = 2 \\
        2z = 2          \\
        x + y = 2
    \end{cases}
\end{align*}

This can be expressed as the matrix equation:

\begin{align*}
    \begin{pmatrix}
        3 & -2 & 1 \\
        0 & 0  & 2 \\
        1 & 1  & 0
    \end{pmatrix}
    \begin{pmatrix}
        x \\
        y \\
        z
    \end{pmatrix}
    =
    \begin{pmatrix}
        2 \\
        2 \\
        2
    \end{pmatrix}
\end{align*}

Let $A$ be the coefficient matrix and $b$ be the constant vector. Then, we can solve for $x$, $y$, and $z$ using Cramer's rule:

\begin{align*}
    \det(A)   & = 0 - 6 - 0 - 4 + 0 - 0 = -10                      \\
    \det(A_1) & = \det \begin{pmatrix}
                           2 & -2 & 1 \\
                           2 & 0  & 2 \\
                           2 & 1  & 0
                       \end{pmatrix} = 0 - 4 + 0 - 8 + 2 - 0 = -10 \\
    \det(A_2) & = \det \begin{pmatrix}
                           3 & 2 & 1 \\
                           0 & 2 & 2 \\
                           1 & 2 & 0
                       \end{pmatrix} = 2 - 6 + 0 - 0 + 2 - 0 = -2  \\
    \det(A_3) & = \det \begin{pmatrix}
                           3 & -2 & 2 \\
                           0 & 0  & 2 \\
                           1 & 1  & 2
                       \end{pmatrix} = 4 - 0 - 0 - 0 + 0 - 4 = 0
\end{align*}

Using Cramer's rule, we get:

\begin{align*}
    x & = \frac{\det(A_1)}{\det(A)} = \frac{-10}{-10} = 1          \\
    y & = \frac{\det(A_2)}{\det(A)} = \frac{-2}{-10} = \frac{1}{5} \\
    z & = \frac{\det(A_3)}{\det(A)} = \frac{0}{-10} = 0
\end{align*}

Therefore, the solution to the system of linear equations is $x = 1$, $y = \frac{1}{5}$, and $z = 0$.

\subsubsection*{Additional Examples}
\begin{enumerate}
    \item Determine if the following system of linear equations has a unique solution:
          \begin{align*}
              \begin{pmatrix}
                  3 & -2 & 1 \\
                  2 & 0  & 2 \\
                  1 & 1  & 2
              \end{pmatrix}
              \begin{pmatrix}
                  x \\
                  y \\
                  z
              \end{pmatrix}
              =
              \begin{pmatrix}
                  2 \\
                  2 \\
                  2
              \end{pmatrix}
          \end{align*}

          \begin{align*}
              \det(A) & = 0 - 6 + 8 - 4 + 2 + 0 = 0
          \end{align*}
          Since the determinant of the coefficient matrix is 0, the system does not have a unique solution.

    \item Compute the determinant of the following matrix and determine if it is invertible:
          \begin{align*}
              \begin{pmatrix}
                  1 & 1  & 0  & 1 \\
                  4 & 1  & -1 & 0 \\
                  2 & -1 & 1  & 2
              \end{pmatrix}
          \end{align*}

          We have shown in a previous example that the determinant of this matrix is $-3 \neq 0$. Therefore, the matrix is invertible.

    \item Solve the following system of linear equations using any method:
          \begin{align*}
              \begin{cases}
                  0x + ay + bz = 1 \\
                  cx + 0y + dz = 2 \\
                  ex + fy + 0z = 3
              \end{cases}
          \end{align*}

          This system can be expressed as:
          \begin{align*}
              \begin{pmatrix}
                  0 & a & b \\
                  c & 0 & d \\
                  e & f & 0
              \end{pmatrix}
              \begin{pmatrix}
                  x \\
                  y \\
                  z
              \end{pmatrix}
              =
              \begin{pmatrix}
                  1 \\
                  2 \\
                  3
              \end{pmatrix}
          \end{align*}

          Assuming that the coefficient matrix is invertible (i.e., its determinant is non-zero), we can solve for $x$, $y$, and $z$ using the inverse matrix method:

          \begin{align*}
              \begin{pmatrix}
                  x \\
                  y \\
                  z
              \end{pmatrix}
               & = \begin{pmatrix}
                       0 & a & b \\
                       c & 0 & d \\
                       e & f & 0
                   \end{pmatrix}^{-1}
              \begin{pmatrix}
                  1 \\
                  2 \\
                  3
              \end{pmatrix}
          \end{align*}

          The solution can be computed by finding the inverse of the coefficient matrix and multiplying it with the constant vector.
\end{enumerate}

\section{Vector Spaces}

A vector space is a mathematical structure that generalizes the notion of vectors in Euclidean space. Let $\mathbf{E}$ be a non-empty set and $\mathbb{K}$ be a scalar field (such as the real numbers $\mathbb{R}$ or the complex numbers $\mathbb{C}$).

We designate the elements of $\mathbf{E}$ as vectors. Two operations are defined on $\mathbf{E}$:

1. Vector addition: An internal binary operation '+' that maps two vectors in $\mathbf{E}$ to another vector in $\mathbf{E}$.
2. Scalar multiplication: An external binary operation '.' that maps a scalar in $\mathbb{K}$ and a vector in $\mathbf{E}$ to another vector in $\mathbf{E}$.

For $\mathbf{E}$ to be a vector space over the scalar field $\mathbb{K}$, the following axioms must be satisfied:

\begin{enumerate}
    \item Commutativity of vector addition: For all $\vec{u}$, $\vec{v}$ in $\mathbf{E}$, $\vec{u} + \vec{v} = \vec{v} + \vec{u}$.
    \item Associativity of vector addition: For all $\vec{u}$, $\vec{v}$, $\vec{w}$ in $\mathbf{E}$, $(\vec{u} + \vec{v}) + \vec{w} = \vec{u} + (\vec{v} + \vec{w})$.
    \item Existence of an additive identity: There exists a unique vector $\vec{0}$ in $\mathbf{E}$, called the zero vector, such that for all $\vec{v}$ in $\mathbf{E}$, $\vec{v} + \vec{0} = \vec{v}$.
    \item Existence of additive inverses: For every $\vec{v}$ in $\mathbf{E}$, there exists a unique vector $-\vec{v}$ in $\mathbf{E}$, called the additive inverse of $\vec{v}$, such that $\vec{v} + (-\vec{v}) = \vec{0}$.
    \item Commutativity of scalar multiplication: For all $\alpha$ in $\mathbb{K}$ and $\vec{v}$ in $\mathbf{E}$, $\alpha (\vec{v}) = (\alpha \vec{v})$.
    \item Associativity of scalar multiplication: For all $\alpha$, $\beta$ in $\mathbb{K}$ and $\vec{v}$ in $\mathbf{E}$, $(\alpha \beta) \vec{v} = \alpha (\beta \vec{v})$.
    \item Distributivity of scalar multiplication over vector addition: For all $\alpha$ in $\mathbb{K}$ and $\vec{u}$, $\vec{v}$ in $\mathbf{E}$, $\alpha (\vec{u} + \vec{v}) = \alpha \vec{u} + \alpha \vec{v}$.
    \item Distributivity of scalar multiplication over field addition: For all $\alpha$, $\beta$ in $\mathbb{K}$ and $\vec{v}$ in $\mathbf{E}$, $(\alpha + \beta) \vec{v} = \alpha \vec{v} + \beta \vec{v}$.
    \item Existence of a scalar multiplicative identity: For every $\vec{v}$ in $\mathbf{E}$, there exists a unique scalar $1$ in $\mathbb{K}$, called the multiplicative identity, such that $1 \vec{v} = \vec{v}$.
\end{enumerate}

Some examples of vector spaces include:

- $\mathbb{R}^n$: The set of all $n$-tuples of real numbers, with the usual vector addition and scalar multiplication operations.
- $\mathcal{P}_n(\mathbb{R})$: The set of all polynomials of degree at most $n$ over the real numbers, with polynomial addition and scalar multiplication.
- $\mathcal{M}_{m \times n}(\mathbb{R})$: The set of all $m \times n$ matrices with real entries, with matrix addition and scalar multiplication.

Vector spaces have many important properties and applications in various areas of mathematics, physics, and engineering, such as linear algebra, functional analysis, and quantum mechanics.

\subsubsection*{Example}

Show that the set $\mathcal{P}_2(\mathbb{R})$ of all polynomials of degree at most 2 with real coefficients, together with the usual polynomial addition and scalar multiplication operations, forms a vector space over the real numbers $\mathbb{R}$.

Let $\mathbf{E} = \mathcal{P}_2(\mathbb{R})$ and $\mathbb{K} = \mathbb{R}$. We need to verify that the axioms for a vector space are satisfied.

1. Commutativity of vector addition: For any $p(x) = a_2 x^2 + a_1 x + a_0$ and $q(x) = b_2 x^2 + b_1 x + b_0$ in $\mathbf{E}$,
\begin{align*}
    p(x) + q(x) & = (a_2 x^2 + a_1 x + a_0) + (b_2 x^2 + b_1 x + b_0) \\
                & = (a_2 + b_2) x^2 + (a_1 + b_1) x + (a_0 + b_0)     \\
                & = (b_2 + a_2) x^2 + (b_1 + a_1) x + (b_0 + a_0)     \\
                & = q(x) + p(x)
\end{align*}
Therefore, polynomial addition is commutative.

2. Associativity of vector addition: For any $p(x)$, $q(x)$, $r(x)$ in $\mathbf{E}$,
\begin{align*}
    (p(x) + q(x)) + r(x) & = ((a_2 + b_2) x^2 + (a_1 + b_1) x + (a_0 + b_0)) + (c_2 x^2 + c_1 x + c_0) \\
                         & = ((a_2 + b_2) + c_2) x^2 + ((a_1 + b_1) + c_1) x + ((a_0 + b_0) + c_0)     \\
                         & = (a_2 + (b_2 + c_2)) x^2 + (a_1 + (b_1 + c_1)) x + (a_0 + (b_0 + c_0))     \\
                         & = p(x) + (q(x) + r(x))
\end{align*}
Therefore, polynomial addition is associative.

3. Existence of an additive identity: The polynomial $0(x) = 0$ is the additive identity, since for any $p(x) = a_2 x^2 + a_1 x + a_0$ in $\mathbf{E}$,
\begin{align*}
    p(x) + 0(x) & = (a_2 x^2 + a_1 x + a_0) + 0 \\
                & = a_2 x^2 + a_1 x + a_0       \\
                & = p(x)
\end{align*}

4. Existence of additive inverses: For any $p(x) = a_2 x^2 + a_1 x + a_0$ in $\mathbf{E}$, the additive inverse is $-p(x) = (-a_2) x^2 + (-a_1) x + (-a_0)$, since
\begin{align*}
    p(x) + (-p(x)) & = (a_2 x^2 + a_1 x + a_0) + ((-a_2) x^2 + (-a_1) x + (-a_0)) \\
                   & = (a_2 + (-a_2)) x^2 + (a_1 + (-a_1)) x + (a_0 + (-a_0))     \\
                   & = 0(x)
\end{align*}

5. Commutativity of scalar multiplication: For any $\alpha$ in $\mathbb{R}$ and $p(x) = a_2 x^2 + a_1 x + a_0$ in $\mathbf{E}$,
\begin{align*}
    \alpha p(x) & = \alpha (a_2 x^2 + a_1 x + a_0)                     \\
                & = (\alpha a_2) x^2 + (\alpha a_1) x + (\alpha a_0)   \\
                & = ((\alpha a_2) x^2 + (\alpha a_1) x + (\alpha a_0)) \\
                & = p(\alpha x)
\end{align*}
Therefore, scalar multiplication is commutative.

6. Associativity of scalar multiplication: For any $\alpha$, $\beta$ in $\mathbb{R}$ and $p(x) = a_2 x^2 + a_1 x + a_0$ in $\mathbf{E}$,
\begin{align*}
    (\alpha \beta) p(x) & = (\alpha \beta) (a_2 x^2 + a_1 x + a_0)                                   \\
                        & = ((\alpha \beta) a_2) x^2 + ((\alpha \beta) a_1) x + ((\alpha \beta) a_0) \\
                        & = \alpha ((\beta a_2) x^2 + (\beta a_1) x + (\beta a_0))                   \\
                        & = \alpha (\beta p(x))
\end{align*}
Therefore, scalar multiplication is associative.

7. Distributivity of scalar multiplication over vector addition: For any $\alpha$ in $\mathbb{R}$ and $p(x)$, $q(x)$ in $\mathbf{E}$,
\begin{align*}
    \alpha (p(x) + q(x)) & = \alpha ((a_2 x^2 + a_1 x + a_0) + (b_2 x^2 + b_1 x + b_0))                                              \\
                         & = \alpha ((a_2 + b_2) x^2 + (a_1 + b_1) x + (a_0 + b_0))                                                  \\
                         & = (\alpha (a_2 + b_2)) x^2 + (\alpha (a_1 + b_1)) x + (\alpha (a_0 + b_0))                                \\
                         & = ((\alpha a_2) x^2 + (\alpha a_1) x + (\alpha a_0)) + ((\alpha b_2) x^2 + (\alpha b_1) x + (\alpha b_0)) \\
                         & = \alpha p(x) + \alpha q(x)
\end{align*}
Therefore, scalar multiplication is distributive over vector addition.

8. Distributivity of scalar multiplication over field addition: For any $\alpha$, $\beta$ in $\mathbb{R}$ and $p(x) = a_2 x^2 + a_1 x + a_0$ in $\mathbf{E}$,
\begin{align*}
    (\alpha + \beta) p(x) & = ((\alpha + \beta) a_2) x^2 + ((\alpha + \beta) a_1) x + ((\alpha + \beta) a_0)                       \\
                          & = ((\alpha a_2) x^2 + (\alpha a_1) x + (\alpha a_0)) + ((\beta a_2) x^2 + (\beta a_1) x + (\beta a_0)) \\
                          & = (\alpha a_2 + \beta a_2) x^2 + (\alpha a_1 + \beta a_1) x + (\alpha a_0 + \beta a_0)                 \\
                          & = \alpha p(x) + \beta p(x)
\end{align*}
Therefore, scalar multiplication is distributive over field addition.

9. Existence of a scalar multiplicative identity: The scalar $1$ in $\mathbb{R}$ is the multiplicative identity, since for any $p(x) = a_2 x^2 + a_1 x + a_0$ in $\mathbf{E}$,
\begin{align*}
    1 p(x) & = (1 a_2) x^2 + (1 a_1) x + (1 a_0) \\
           & = a_2 x^2 + a_1 x + a_0             \\
           & = p(x)
\end{align*}

Since all the axioms for a vector space are satisfied, $\mathcal{P}_2(\mathbb{R})$ together with the usual polynomial addition and scalar multiplication operations forms a vector space over the real numbers $\mathbb{R}$.

\section{Linear Transformations}

\pagebreak

\section{Multi-variate Calculus}

\subsection{Function, limits and continuity}

To advance into Multi-variate Calculus, a more rigorous definition of Function, limits and continuity is required.

\subsubsection{Intervals}

Here are some tasty interval notations:

\begin{align}
    \textrm{Assume all describes an interval between real numbers.} \\
    [a,b] \implies \textrm{A to b inclusive of both points}
\end{align}

\subsection{Examples}

The definition and proofs of limits and continuity in multivariable calculus are similar to those in single-variable calculus. Here are some examples of limits and continuity in multivariable calculus:

\begin{align}
    f(x,y) := 
    \begin{cases}
    \frac{x^2 y}{x^4 + y^2} & (x,y) \neq 0 \\
    0 & (x,y) = 0 \\
    \end{cases} \\
\end{align}

\begin{align}
    \lim_{x\to0} f(x) &= \frac{xe^{-x^{2}-y^{2}}}{e^x - 1} \\
    &= (xe^{-(x^2+y^2)})(\frac{1}{e^x - 1}) = (\frac{x}{e^{x^2+y^2}})(\frac{1}{e^x - 1}) \\
    &= \frac{x}{e^{x^2+y^2+x} + e^{x^2+y^2}} \\
    \lim_{x\to0} f(x) &=\frac{0}{2e^{y^2}} = 0\\ 
\end{align}



\end{document}